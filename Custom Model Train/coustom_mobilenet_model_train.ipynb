{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11722,
     "status": "ok",
     "timestamp": 1758036636065,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "LGoxbPDLgvZ0",
    "outputId": "90dcbb2f-2e7c-4c89-fc2f-eb6e159fad9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3115 images belonging to 36 classes.\n",
      "Found 351 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define paths for the dataset\n",
    "train_dir = './dataset/train'\n",
    "val_dir = './dataset/validation'\n",
    "\n",
    "# Create ImageDataGenerators for training and validation data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize the images\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for validation\n",
    "\n",
    "# Load images from the directories and apply the transformations\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # Resize images to 224x224\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class classification\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3342,
     "status": "ok",
     "timestamp": 1758036640495,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "I-FNM1GahFF-",
    "outputId": "bf718a1e-2ac5-4709-bdd4-a5153aac6c61"
   },
   "outputs": [],
   "source": [
    "# Load MobileNetV2 with pre-trained ImageNet weights, excluding the top layers\n",
    "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during the initial training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')  # Output layer based on the number of classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2406869,
     "status": "ok",
     "timestamp": 1758039047365,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "58HkBhdMhKc9",
    "outputId": "4105e984-c7cb-4362-97e9-c3c9a77f34a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1A SEM 7\\End\\Computer Vision\\Project\\my_flask_app\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 7/97\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:15\u001b[0m 4s/step - accuracy: 0.1754 - loss: 3.5252"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1A SEM 7\\End\\Computer Vision\\Project\\my_flask_app\\venv\\Lib\\site-packages\\PIL\\Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3s/step - accuracy: 0.6098 - loss: 1.3886 - val_accuracy: 0.8531 - val_loss: 0.4887\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/97\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 982ms/step - accuracy: 0.6875 - loss: 1.1452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1A SEM 7\\End\\Computer Vision\\Project\\my_flask_app\\venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 183ms/step - accuracy: 0.6875 - loss: 1.1452 - val_accuracy: 0.8562 - val_loss: 0.4760\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 2s/step - accuracy: 0.7788 - loss: 0.7036 - val_accuracy: 0.8875 - val_loss: 0.3189\n",
      "Epoch 4/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 148ms/step - accuracy: 0.7812 - loss: 0.8529 - val_accuracy: 0.8875 - val_loss: 0.3260\n",
      "Epoch 5/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 3s/step - accuracy: 0.8271 - loss: 0.5227 - val_accuracy: 0.9281 - val_loss: 0.2529\n",
      "Epoch 6/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 167ms/step - accuracy: 0.9375 - loss: 0.4806 - val_accuracy: 0.9125 - val_loss: 0.2820\n",
      "Epoch 7/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 2s/step - accuracy: 0.8476 - loss: 0.4429 - val_accuracy: 0.9062 - val_loss: 0.2878\n",
      "Epoch 8/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step - accuracy: 0.7812 - loss: 0.6624 - val_accuracy: 0.9094 - val_loss: 0.2766\n",
      "Epoch 9/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.8790 - loss: 0.3558 - val_accuracy: 0.9125 - val_loss: 0.2692\n",
      "Epoch 10/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 139ms/step - accuracy: 0.9688 - loss: 0.1770 - val_accuracy: 0.9312 - val_loss: 0.2674\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the training and validation data\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 502069,
     "status": "ok",
     "timestamp": 1758039549437,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "uUG-LRehhNMk",
    "outputId": "2052af88-8d77-4fd3-8501-2c4ccd7d5652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 3s/step - accuracy: 0.8141 - loss: 0.5764 - val_accuracy: 0.9187 - val_loss: 0.2544\n",
      "Epoch 2/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 136ms/step - accuracy: 0.9375 - loss: 0.3348 - val_accuracy: 0.9281 - val_loss: 0.2421\n",
      "Epoch 3/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 2s/step - accuracy: 0.8716 - loss: 0.4230 - val_accuracy: 0.9156 - val_loss: 0.2292\n",
      "Epoch 4/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 158ms/step - accuracy: 0.8125 - loss: 0.4483 - val_accuracy: 0.9219 - val_loss: 0.2354\n",
      "Epoch 5/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 2s/step - accuracy: 0.8797 - loss: 0.3934 - val_accuracy: 0.9406 - val_loss: 0.1769\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the last few layers of MobileNetV2 for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune only the top 10 layers of the base model\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Re-compile the model after unfreezing layers\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Use a smaller learning rate for fine-tuning\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model\n",
    "history_fine_tune = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43668,
     "status": "ok",
     "timestamp": 1758039599320,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "I_yFqIZihQbk",
    "outputId": "5898d37f-d5fb-4235-f7fa-c58639e14f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.9288 - loss: 0.2227\n",
      "Validation accuracy: 92.88%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1758040913850,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "pyDB6sewhRZU",
    "outputId": "e17ac1e7-5c95-47d9-a223-a47f2b590d8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=tf\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "# In Colab\n",
    "model.save('./custom_mobilenet_model.h5', save_format='tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758039626055,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "SlSCn36YyVTM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "error",
     "timestamp": 1758040849566,
     "user": {
      "displayName": "Chathurya Aralugaswaththa",
      "userId": "06747632246256508426"
     },
     "user_tz": -330
    },
    "id": "_LSSPgMRjF-V",
    "outputId": "a65de987-a092-44b3-d328-85c2f6a64d38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 986ms/step\n",
      "1. pear: 1.00\n",
      "2. apple: 0.00\n",
      "3. orange: 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define custom class labels\n",
    "class_labels = [\n",
    "    'apple', 'banana', 'beetroot', 'bell pepper', 'cabbage', 'capsicum', 'carrot',\n",
    "    'cauliflower', 'chilli pepper', 'corn', 'cucumber', 'eggplant', 'garlic', 'ginger',\n",
    "    'grapes', 'jalepeno', 'kiwi', 'lemon', 'lettuce', 'mango', 'onion', 'orange',\n",
    "    'paprika', 'pear', 'peas', 'pineapple', 'pomegranate', 'potato', 'raddish',\n",
    "    'soy beans', 'spinach', 'sweetcorn', 'sweetpotato', 'tomato', 'turnip', 'watermelon'\n",
    "]\n",
    "model = load_model('custom_mobilenet_model.h5')\n",
    "\n",
    "\n",
    "# Load the image for prediction\n",
    "img_path = './images/Image_32.jpg'\n",
    "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Sort the predictions and get the top 3\n",
    "top_indices = predictions[0].argsort()[-3:][::-1]  # Get the indices of top 3 predictions\n",
    "top_predictions = [(class_labels[i], predictions[0][i]) for i in top_indices]\n",
    "\n",
    "# Print the top 3 predictions\n",
    "for i, (label, score) in enumerate(top_predictions):\n",
    "    print(f\"{i+1}. {label}: {score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKJ7660DpoDTn47es8wXxF",
   "gpuType": "T4",
   "mount_file_id": "1ZDOrKR_czQyaBp5G3eZGcDipXA16_iBF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
